# scripts/collect_and_merge.py

import pandas as pd
import glob
import os
from tqdm import tqdm
import shutil
import json

# --- é…ç½® ---
INPUT_BASE_DIR = "all_data"
OUTPUT_KDATA_DIR = "final_kdata"
OUTPUT_MONEYFLOW_DIR = "final_moneyflow"
QC_REPORT_FILE = "data_quality_report.json"

# run_quality_check å‡½æ•°ä¿æŒä¸å˜

def collect_and_merge_data(data_type, output_dir):
    print("\n" + "="*50)
    print(f"ğŸš€ å¼€å§‹æ”¶é›†å’Œå¤„ç† {data_type} æ•°æ®...")
    
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
    os.makedirs(output_dir)

    # (å…³é”®ä¿®æ­£) æœç´¢è·¯å¾„ä¸å†åŒ…å« "data_slice"
    search_pattern = os.path.join(INPUT_BASE_DIR, "data_part_*", data_type, "*.parquet")
    file_list = glob.glob(search_pattern)
    
    if not file_list:
        print(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½• {data_type} çš„ Parquet æ–‡ä»¶ã€‚")
        return None

    print(f"ğŸ“¦ å…±æ‰¾åˆ° {len(file_list)} ä¸ª {data_type} æ–‡ä»¶ï¼Œå¼€å§‹æ”¶é›†...")
    all_dfs = []
    for src_path in tqdm(file_list, desc=f"æ”¶é›†ä¸­ ({data_type})"):
        try:
            filename = os.path.basename(src_path)
            dest_path = os.path.join(output_dir, filename)
            shutil.copy2(src_path, dest_path)
            all_dfs.append(pd.read_parquet(dest_path))
        except Exception as e:
            print(f"\nâš ï¸ å¤„ç†æ–‡ä»¶ {src_path} å¤±è´¥: {e}")
            
    print(f"âœ… å…¨éƒ¨ {len(file_list)} ä¸ªæ–‡ä»¶å·²æ”¶é›†åˆ° '{output_dir}' ç›®å½•ã€‚")
    
    if all_dfs:
        return pd.concat(all_dfs, ignore_index=True)
    return None

def main():
    kdata_df = collect_and_merge_data("kdata", OUTPUT_KDATA_DIR)
    moneyflow_df = collect_and_merge_data("moneyflow", OUTPUT_MONEYFLOW_DIR)
    
    # run_quality_check(kdata_df, moneyflow_df) # è´¨æ£€å¯ä»¥åç»­å†å®Œå–„
